---
layout: post
title: cpu高速缓存
description: ""
category: 硬件
tags: [cache, 硬件]
---
{% include JB/setup %}

* toc
{:toc}

<hr />

## 为什么缓存策略有效

理解CPU高速缓存关键是理解CPU内存系统的瓶颈在哪里以及局部性原理。为了理解CPU内存系统的瓶颈，我们进行如下估算：

估算1：假若一个CPU正在跑一个循环代码，代码指令都已经缓存在IL1中，因此取指不需要访存。假设每条指令都需要从内存中读取数据，CPU位宽为64bit也就是8B。那么按照现在CPU每个逻辑核至少10K MIPS也就是10GIPS计算计算，单个逻辑核需要的带宽为80GB/s。当然这个估算是非常离谱的，因为并不是每条指令都需加载数据，再一个由于局部性原理，也不一定需要从内存中加载数据。

估算2：根据L1，L2，L3缓存带宽来计算。以Sandy Bridge的i7-2920XM为例，根据[这里][0]提供的方法，查询[《Intel® 64 and IA-32 Architectures Optimization Reference Manual》][5]的Table 2-25.  Lookup Order and Load Latency，计算出单个核心需要的L3缓存带宽为2.5Gx32B=80GB/s。而i7-2920XM的[最大内存带宽仅为25.6GB][[1]]。

根据上面两种估算，都可以得出一个结论：真正IO密集型的计算情景下，CPU需要的带宽是远大于内存能提供的带宽的。因此无论如何缓存策略是不能解决带宽问题的。

但是因为DDR内存的构造原因，内存的访问延迟大约为50-100ns，远远大于一次[CPU的指令执行时间][2]1ns以内。

而通常情况下，CPU访存具有[局部性][3]且是时间与空间上的局部性，也就是短时间内反复访问同一块内存以及访问的内存在物理上连续存储。除非上面所讲的IO密集型，大部分情况下CPU访存的特点“少量而急切”。
因此可以通过预取后面内存的方式加快内存访问。而真正IO密集型只要不是完全的随机访问，缓存的存在也不会导致内存访问变慢。

关于缓存策略为什么有效的资料还可以参考[为什么在 CPU 中要用 Cache 从内存中快速提取数据][4]。

## 缓存的结构




通常，cpu缓存是一个多级结构，一级缓存最小，但延迟也小，几乎与指令同频率存取。二级缓存、三级缓存依次增大但延迟也逐渐变高。

![img](/assets/resources/cpu_cache_levels.jpg)

如上图，最后一级共享，意味着维持缓存一致的时候不必写回内存，只需要写回到最后一级缓存(Last Level Cache又称LLC)。


之所以设计成多级结构是片上空间限制、性能、价格、命中率等多方面的考量。组成一级缓存的晶体管较为昂贵，而且由于局部性原理，随着L1缓存的增大命中率的增加逐渐减小。因此与其增加L1缓存的容量，不如用更大的较为便宜但延迟稍高的L2缓存。L2缓存虽然延迟较高，但是L1不命中的概率只要足够低，那么L2缓存的较高的延迟就不成问题。[图片来源][6]。

![img](/assets/resources/L1-L2Balance.png)

整个缓存体系的大小受限于片上面积。如下图，中间的部分就是Haswell-E芯片上20M的L3缓存。[图片来源][6]。

![img](/assets/resources/haswell-e-die-shot-high-res.jpg){:width="100%"}

根据下级缓存是否包含上级缓存，可以分为inclusive与exclusive设计。

缓存是以cacheline为单位进行操作的，cacheline的大小随CPU架构的不同而不同。intel CPU的cacheline大小可通过cpuid指令或者查询
[intel优化手册][5]获得，通常为64bytes。

一个cacheline与内存的映射关系通常有三种组织方法，即所谓的直接相连、全相连与组相连。

令cacheline大小为$$2^{cl}$$ 字节。缓存大小为$$2^c \times S_{cl}$$字节。令内存地址空间（可以是虚拟地址空间也可以是物理地址空间，取决于下面要讲的地址转换方式）为$$2^m \times 2^{cl}$$字节。缓存与内存都按照cacheline的大小分成一个个连续的block。

内存内一个索引为mi的block的地址为$$mi \times 2^{cl}$$，显然mi的取值为{0...$$2^m$$}，该地址最低cl位必然是0，也就是$$2^{cl}$$对齐的。

在直接相连的映射方式下，该内存block映射到cache中的索引为$$ mod(mi,2^{cl})$$，mod为取模。[图片来源][7]。

![img](/assets/resources/cpu_cache_direct_map.gif)

直接相连的方法查询块，但是命中率低。如何理解这个命中率低呢？


在全相连的映射方式下，该内存block可以可以映射到cache中的任意一个block。[图片来源][7]。

![img](/assets/resources/cpu_cache_full_map.gif)

全相连映射查询慢，因为要最坏情况下要遍历整个缓存才能知道缓存是否命中，一般32KB的一级缓存64B的cacheline，包含512个条目，逐个比较的开销是比较大的。

折中的方式是组相连映射方式。这种映射方式下内存地址空间仍然按照block来划分，但是缓存内将cacheline分成组，每组包含若干cacheline。令$$2^s$$为组数，一组内有$$2^b$$个cacheline，则$$2^s \times 2^b \times = 2^c$$，也就是$$c=s+b$$。在映射的时候，内存中的一个block首先按照缓存内组数取模，$$mod(mi, 2^s)$$确定组号，这一步类似直接映射。然后该block可以映射到该组内的任意一个cacheline，这一步类似全相连。也就是所谓的组外直接相连，组内全相连。[图片来源][7]。

![img](/assets/resources/cpu_cache_set_map.gif)

缓存内的一个cacheline被包含在一个cache entry内。显然，一个cacheline可以对应内存地址空间中的多个block，为了确定该cacheline到底是哪个block，cacheline上还要附加一个tag。**在组相连下，确定一个组号后的cacheline可能对应内存地址空间中的$$2^{m-s}$$个block中任意一个，这个tag就是该block地址高m-s位**。如图

![img](/assets/resources/cpu_cache_tag_address.png){:width="100%"}

附加上tag后，一个cache entry往往还有一个flags bits。完整的cache entry结构示意图如下：

![img](/assets/resources/cpu_cache_entry.png)

所以当拿到一个内存地址后，如何确定该地址所在内存是否在缓存中内，首先根据s bits找到缓存内对应的cacheline或者组，然后根据tag确认缓存中是否是想要的cacheline。看起来似乎tag必然是物理地址，其实不然，后面会讲4种地址转换方式。

关于flag bits的说明参见[百科][8]。指令缓存一般需要1位flag：valid flag。数据缓存一般需要2位flag：valid flag与dirty flag。

我的计算机的Intel Xeon 1230 V2上从cpuid上查得的cache信息：

![img](/assets/resources/cpu_cache_e3_1230_caches.png)



## 缓存地址转换方式

上一小节提到三段地址：一段用来做tag，一段用来做index，一段用来索引cacheline内偏移。看起来这三段地址是组成一个完整的虚拟地址或者物理地址。其实不然，存在tag与index分别是虚拟地址与物理地址的情况，也就是存在PIPT、VIPT、PIVT、VIVT四种确定cacheline的方式。理解这种混杂的情况，首先要认识到尽管物理内存可能远小于虚拟地址空间，但物理地址空间必然是跟虚拟地址空间一样大小的，我们用内存地址空间来统指虚拟地址空间与物理地址空间。也就是tag、index、cacheline内偏移无论怎么组合，整个地址空间是不变的。

以下讨论都是基于x86平台。在继续之前，让我们先回忆下在x86平台下虚拟地址与物理地址的转换方式：物理地址空间与虚拟地址空间都分成了相同大小的页，通常是4k，也存在2M与1G的大页模式。虚拟地址空间中的页与物理地址空间中的页一一对应。

这里我们用一种演进的方式来理解4种地址映射方式，这种方法仅仅是帮助理解，跟历史上真实的演进过程没有任何关系。

现在我们有一个32KB的8路组相连的cache，有一个4GB的进程线性（虚拟）地址空间。

考虑在VIVT的映射方式下，CPU需要访问某个虚拟地址x。
因为索引是虚拟地址，所以可以直接去Cache找到那一组cacheline，然后依次对比每个虚拟地址的tag，判断对应的cacheline是否在缓存中。

一切看起来很美好。但是系统内是存在多个进程的，多个进程之间可能共享一些物理内存，如果相同的物理内存被映射到了不同的虚拟地址。
那么使用虚拟地址索引就会映射到不同的cacheline中。这会给缓存一致性协议带来麻烦。这个问题称之为synonym问题，或者别名问题。

	synonym问题：同一个物理地址对应不同的虚拟地址导致cache中存在同一物理地址的多份，又称别名问题

	问题形成条件：
	 1. 进程间共享内存（往往是系统内存）没有统一映射  
	 2. 使用虚拟地址做索引

	影响：给缓存一致性协议带来问题。

	解决方法：

	方法一：操作系统或者cpu能保证任何一块内存都只能属于一个进程（共享内存属于一个系统进程）

	方法二：使用物理地址做索引

	方法三: 页表足够大，使得索引地址+cacheline偏移小于等于页表。这样的话所以地址在页内偏移地址中，从而虚拟地址索引与物理地址索引一样
	保证了同一个物理地址有相同的索引。因为此时虚拟地址做索引等同于物理地址做索引，因此本质上等于方法二。

	注意这个问题通过物理地址做tag不能解决，tag是用来区分不同的物理内存，而这里的是相同的物理内存不同的index。

上面的方法三如图：
![img](/assets/resources/cpu_cache_alias_solution.png){:width="100%"}

其中方法二使用物理地址做索引就是PIVT。
intel使用的是方法三，该方法的取巧，使用虚拟地址做索引因而不需要地址转换。但intel使用的是物理地址tag，下面会讲。

Ok，解决了synonym问题之后，一切看起来恢复正常了，但没这么简单，多个进程的虚拟地址空间往往是相同的，比如x86 32位平台下linux系统中，每个进程的虚拟地址空间都是0到4G，但其物理内存肯定不一样，因为是不同的进程嘛。所以有可能相同的虚拟地址被映射到了不同的物理内存上。当发生进程切换的时候，若是下一个执行的进程按照虚拟地址索引去缓存中查找内容，有可能找到上一个进程的物理内存。这个问题被称为homonym问题。

	Homonym：同一个虚拟地址映射到不同的物理地址

	形成条件：1. 进程共用同一个线性地址空间导致的
			  2. 使用虚拟地址做tag
			  

	影响：共用虚拟地址空间的进程虚拟地址一样的时候无法凭index与tag互相区分，上下文切换的时候要先清空cache。如果没有这个问题，上下文切换的时候不需要清空，因为会引发cache miss。

	解决方法：
	方法一. 把各进程线性空间隔离
	方法二. 使用物理地址做tag

	可以给cacheline附加上进程号或者物理地址，但还是无法解决synonym问题。

一般CPU的虚拟地址空间与物理地址空间一致，比如x86 32为都是4G空间，隔离显然会限制当个进程的地址空间大小，64位系统下倒是有可能。

方法二就是intel的做法。上面讲synonym问题讲到，intel取巧将cacheline内偏移与cache索引控制在4K页的地址宽度以内，从而避免了别名问题。因此intel的VIPT做法兼顾了查询效率，避免了synonym与homonym问题。当然这是intel L1cache的做法。[L2与L3应该是PIPT][10]。因为无论L1命中与否，物理地址肯定已经查出来了，继续使用这个问题地址去检索L2并没有任何效率问题，反而能避免VIVP的各种问题。

intel这种取巧的做法代价是限制了L1缓存的大小，但没有完全限制，依然可以通过增加路数扩大L1，但是组内是全相连，增加路数也会受限于查询时间。因为4K页的页内偏移地址为12位。1路组相连下这种做法最大cache为6位cacheline内偏移+6位索引为4KB。增加路数等倍数增加cache。因而8路组相连是32KB。只要最小页大小依然是4KB且是8路组相连，intel的这种做法L1cache就不可能突破每核心32KB。


上面可以看到VI 与VT分别有可能导致synonym与homonym问题，那直接用PIPT就可以避免这两个问题，代价是每次查询缓存都需要做地址转换，尽管有TLB且TLB命中，这个代价依然有些高。其实intel的做法也可以理解为PIPT，只是这个PI不需要经过转换。

如何理解这个代价有多大呢？在理解这问题之前我们先看下intel的做法，虚拟地址索引与物理地址索引一样，使得查询缓存时通过index查询缓存与通过tag查询物理地址并行处理，使得L1与指令工作在同一频率。在TLB命中的情况下，合理假设是一个地址在查询cache时，通过TLB且命中查询地址所对应的物理页基地址与确定cacheline所在的组花费同样的时间，那么如果改成串行，合理的推测是L1需要等待原来2倍的cycles。因此这种并行的做法大约提高了100%的性能。

## 带缓存与TLB的CPU内存访问流程

上一节讲查询cache需要将虚拟地址转换为物理地址，因而需要查询页表，而多级页表需要多次访存，访存的延迟约为50ns到100ns，代价相当大。现代CPU为了加速这一过程会内置[TLB][9]。

TLB类似于缓存也是一个多级组相连结构，TLB的结构参数也可以通过cpuid或者查询[手册][5]获得。此处不再展开，等到讲cpu内存模式的时候再去细说。

带有缓存与TLB的CPU一次访存地址转换流程可以参考下图：[图片来源][7]

![img](/assets/resources/cpu_cache_visit_flow.png){:width="100%"}

需要注意的是这图中为了表示一个完整的地址，没有画出VI与PT并行处理。这图是一个地址转换流程，不能当做处理流程。

## 缓存访问延时与内存延时差距

具体数值取决于cpu架构与内存。但是有一些经验数据可以参考。

[数据一 Intel Skylake benchmark][11]。

根据上面的数据

```
L1 Data Cache Latency = 4 cycles for simple access via pointer
L1 Data Cache Latency = 5 cycles for access with complex address calculation (size_t n, *p; n = p[n]).
L2 Cache Latency = 12 cycles
L3 Cache Latency = 42 cycles (core 0) (i7-6700 Skylake 4.0 GHz)
L3 Cache Latency = 38 cycles (i7-7700K 4 GHz, Kaby Lake)
RAM Latency = 42 cycles + 51 ns (i7-6700 Skylake)

```

4GHz下L1约为1ns，L2为4ns，L3约为10ns，内存约为61ns。与下面总结的经验数据数量级上基本一致。

[数据二 latency][12]。



按照intel skylake那个数据来计算，假设各级命中率为90，可以计算出平均访问延迟为1.4 ns。惊人的结果！

```
1 * 0.9 + 0.1 * (4 * 0.9 + 0.1 * (10 * 0.9 + 0.1 * 61)) = 1.411 ns。

```

经验上可以认为内存访问延迟为整个三级缓存体系的平均访问延迟的50-100倍。


## 缓存行替换算法

参考[wiki][13]。

## 缓存一致性
如何理解缓存一致性的目的，封装掉缓存层

与同步、内存模型的关系。

缓存一致性是一种最终一致性。

而同步则要求在某个时机必须保证一致性。

缓存一致性是同步与内存的实现基础。
	
## 缓存大小与命中率与性能

参考如下资料即可。

[资料][6]

文章里讲到很有意思的一点，L1缓存命中率95%与97%的性能差距可不止2%，而是14%。


## 内存限制了CPU的吞吐量

本文开头就讲了CPU实际上带宽受限内存带宽，但是CPU本身吞吐量还受限于Last Level Cache。关于
Last Level Cache吞吐量的计算参考如下资料:

[如何计算cache 吞吐量][0]

上面资料涉及的[E5-2687w的最大内存带宽][14]

[该资料的Memory subsystem performance一节][15]


## 高效利用缓存

[缓存读写性能测试][16]

False sharing：所谓的False sharing指的是不同核心上的两个线程共享同一块物理内存的cacheline，当一个核心修改了cacheline内某个数据后，
缓存一致性协议会将其他核心内的这个整个cacheline标记为无效。

上面的测试中有对false sharing的测试，可以看到性能差距巨大。

更过说明参考[资料1][17]，[资料2][18]。


## 其他
推荐阅读：

[How L1 and L2 CPU Caches Work, and Why They’re an Essential Part of Modern Chips][6]

[维基百科CPU CACHE][https://en.wikipedia.org/wiki/CPU_cache]

知乎上有个系列文章可以[参考][https://zhuanlan.zhihu.com/p/31422201]。该系列最求通俗易懂，若有与本文章冲突之处，以本文为准。



[0]:https://software.intel.com/en-us/forums/intel-moderncode-for-parallel-architectures/topic/608964
[1]:https://ark.intel.com/content/www/us/en/ark/products/52237/intel-core-i7-2920xm-processor-extreme-edition-8m-cache-up-to-3-50-ghz.html
[2]:https://software.intel.com/en-us/vtune-help-cpi-rate
[3]:https://en.wikipedia.org/wiki/Locality_of_reference
[4]:https://www.zhihu.com/question/22431522
[5]:https://software.intel.com/sites/default/files/managed/9e/bc/64-ia-32-architectures-optimization-manual.pdf
[6]:https://www.extremetech.com/extreme/188776-how-l1-and-l2-cpu-caches-work-and-why-theyre-an-essential-part-of-modern-chips
[7]:https://my.oschina.net/fileoptions/blog/1630855
[8]:https://en.wikipedia.org/wiki/CPU_cache#Cache_entry_structure
[9]:https://en.wikipedia.org/wiki/Translation_lookaside_buffer
[10]:https://software.intel.com/en-us/forums/software-tuning-performance-optimization-platform-monitoring/topic/748044
[11]:https://www.7-cpu.com/cpu/Skylake.html
[12]:https://gist.github.com/jboner/2841832
[13]:https://en.wikipedia.org/wiki/CPU_cache#Policies
[14]:https://ark.intel.com/content/www/us/en/ark/products/64582/intel-xeon-processor-e5-2687w-20m-cache-3-10-ghz-8-00-gt-s-intel-qpi.html
[15]:https://techreport.com/review/27018/intels-xeon-e5-2687w-v3-processor-reviewed/
[16]:https://github.com/colddrizzle/cache_perf
[17]:https://mechanical-sympathy.blogspot.com/2011/07/false-sharing.html
[18]:https://www.cnblogs.com/cyfonly/p/5800758.html
