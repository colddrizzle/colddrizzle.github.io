自组织映射网络


#大论
som算法是一个无监督学习算法。

假设一个样本有n个特征，每个特征用一个数表示，于是一个样本就可以看做是一个n维向量，或者n维空间中的一个点。使用som算法首先给出竞争层的神经元数目，
竞争层的神经元之间存在空间拓扑关系，这意味着欧式距离较近的神经元相似性更高。然后每一个神经元有一个n维的权重，也就是跟样本维度相同，这个权重代表这个神经元代表的类。

首先随机或者人为指定的方式用一些样本初始化每个神经元的权重。

然后可以开始训练过程，依次将每一个样本喂给每一个神经元，通过欧式距离找出距离该样本最近的神经元，称之为激活，激活的神经元拥有调整自己以及其拓扑空间周围神经元权重的权利。

由于激活神经元是通过比较跟样本的距离得出的，因此称之为竞争策略。

激活神经元调整权重的时候倾向于将周围的神经元的权重设置的与激活神经元的权重更相似，但是会按照距离以及迭代的次数衰减，也就是一种以激活神经元为中心聚拢的倾向。

由于竞争层的神经元数量大大小于样本的数量，于是整个样本空间被迫聚拢在神经元为中心的几个类上。

因此，som算法的本质是将N维空间的点聚类，然后为每个类找一个中心， 然后投射到N维甚至低纬度空间上。或者说是在逼近样本的概率分布。

或者说som算法是在用竞争层神经元的空间拓扑关系来贴合样本的空间拓扑关系，这样想我们就很容易理解下面提到的一个用som来解决旅行商问题的例子了。

想象一个三维空间中分布着几类点，并且竞争网络是2维的，som算法就相当于找到这样一个平面：各类点投射到这个平面上后尽量不相交。

som算法的神奇之处在于渐进逼近的方法、竞争的方法。竞争有点物竞天择的味道，表现在数学上其实就是比较行为，比如数的大小的比较。比如一个水池一边进水一边排水，
进水与排水就处于竞争关系，当进水大于排水时，水池就上涨，反之则下降，可见竞争导致了状态的分化。


[这里](https://zhuanlan.zhihu.com/p/73534694)有一个动图可以直观感受som算法。

#minisom源码

首先，minisom的竞争网络最多是2维的。



#例子

https://www.zhihu.com/question/28046923/answer/499882606

https://github.com/JustGlowing/minisom/tree/master/examples

https://zhuanlan.zhihu.com/p/34121865 这是一个有趣的例子，使用som算法来逼近旅行商问题的最优解。

#som存在的问题与参数调整

根据minisom的代码来看，som算法存在的问题有：
随机初始化的时候过于偏离样本分布，导致有些样本激活的神经元实际上跟样本偏差很大，也就是强制激活

或者随机初始化比较成功，但是随后的邻域半径设置的过大（或者根本就是因为som的竞争层太小，比如只有两个神经元）导致一个激活的神经元修改权重的时候严重影响另一些未激活
神经元的权重，然后其他神经元激活的时候重复这个过分的影响，从而使整个训练过程处于一个反复震荡的状态，训练停止时结果趋向于随机。

问题：竞争层是权重是用随机抽取的样本初始化的。因此竞争层之间的空间关系未必是样本的空间关系，其算法的邻域更新策略不就存在问题了吗？


上面提到竞争层太少导致的问题，竞争层神经元的数目存在一个经验公式$$5\sqrt{3}$$。


som算法的sigma调整很大程度上依赖于样本数量以及分布，并不容易确定。

#其他资料

https://zhuanlan.zhihu.com/p/73930638

https://zhuanlan.zhihu.com/p/300028384

https://www.cnblogs.com/surfzjy/p/7944454.html

https://blog.csdn.net/ye_xiao_yu/article/details/78758998


既然som这么有趣的话，不如再把其他早期的神经网络也了解一下，比如什么自适应共振理论、hopfield网络等。






